{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Requests module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Request get**\n",
    "1. request http get\n",
    "2. deliver data using query parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "url = 'https://www.cktimes.net/news/%EB%84%B7%ED%94%8C%EB%A6%AD%EC%8A%A4-%EC%BA%90%EB%82%98%EB%8B%A4-%EC%A7%80%EC%82%AC-%EC%84%A4%EB%A6%BD-%EA%B3%84%ED%9A%8D/'\n",
    "resp = requests.get(url)\n",
    "resp.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Requaest post**\n",
    "1. request http post\n",
    "2. deliver data using post data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.kangcom.com/member/member_check.asp'\n",
    "data = {\n",
    "    'id' : 'chy2207',\n",
    "    'pwd' : 't'\n",
    "}\n",
    "resp = requests.post(url, data = data)\n",
    "resp.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data in HTTP header**\n",
    "1. header data\n",
    "2. deliver header data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://news.v.daum.net/v/20190728165812603'\n",
    "headers = {\n",
    "    'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.142 Safari/537.36'\n",
    "}\n",
    "\n",
    "resp = requests.get(url, headers=headers)\n",
    "resp.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HTTP response \n",
    "1. Check status_code \n",
    "2. Check property of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://news.v.daum.net/v/20190728165812603'\n",
    "resp = requests.get(url)\n",
    "if resp.status_code == 200:\n",
    "    resp.text\n",
    "    #resp.headers\n",
    "else:\n",
    "    print('error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### USING OPEN API\n",
    "**Using public data potal**\n",
    "(https://www.data.go.kr/)\n",
    "1. Request using API / request key\n",
    "2. Check API specification\n",
    "3. test API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check endpoit (domain address/ service ip)\n",
    "serviceKey = 'GOe7vW0EjFIGLKTPBW2kPET%2FzYRtXB2WJ9i1oHP8PcONPPAgidtCTDBkPskKt2NMbCvTjiN13jAttzmsNXoX1Q%3D%3D'\n",
    "endpoint = 'http://api.visitkorea.or.kr/openapi/service/rest/EngService/areaCode?serviceKey={}&numOfRows=10&pageSize=10&pageNo=1&MobileOS=ETC&MobileApp=AppTest&_type=json'.format(serviceKey)\n",
    "print(endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check parameter\n",
    "endpoint = 'http://api.visitkorea.or.kr/openapi/service/rest/EngService/areaCode?serviceKey={}&numOfRows=10&pageSize=10&pageNo={}&MobileOS=ETC&MobileApp=AppTest&_type=json'.format(serviceKey, 1)\n",
    "resp = requests.get(endpoint)\n",
    "\n",
    "print(resp.status_code)\n",
    "print(resp.text)\n",
    "\n",
    "data = resp.json()\n",
    "# resp.json()\n",
    "\n",
    "print(data['response']['body']['items']['item'][0])\n",
    "# print(data['response']['body']['items']['item'][1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = '''\n",
    "<html>\n",
    "  <head>\n",
    "    <title>BeautifulSoup test</title>\n",
    "  </head>\n",
    "  <body>\n",
    "    <div id='upper' class='test' custom='good'>\n",
    "      <h3 title='Good Content Title'>Contents Title</h3>\n",
    "      <p>Test contents</p>\n",
    "    </div>\n",
    "    <div id='lower' class='test' custom='nice'>\n",
    "      <p>Test Test Test 1</p>\n",
    "      <p>Test Test Test 2</p>\n",
    "      <p>Test Test Test 3</p>\n",
    "    </div>\n",
    "  </body>\n",
    "</html>'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**find function**\n",
    "* search a sepcific html tag\n",
    "* search a tag with specific condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using tag\n",
    "soup.find('h3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find('p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#soup.find('div') #call uper\n",
    "soup.find('div', custom='nice') #call lower\n",
    "# soup.find('div', class_='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using attributes\n",
    "attrs = {'id': 'upper', 'class': 'test'}\n",
    "soup.find('div', attrs=attrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using find_all function\n",
    "soup.find_all('div', class_='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**get_text function**\n",
    "* extract value in tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag = soup.find('h3')\n",
    "print(tag)\n",
    "tag.get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag = soup.find('div',id='upper')\n",
    "print(tag)\n",
    "tag.get_text().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract attribute\n",
    "tag = soup.find('h3')\n",
    "print(tag)\n",
    "tag['title']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**extract data in news**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://news.v.daum.net/v/20190728165812603'\n",
    "resp = requests.get(url)\n",
    "resp.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(resp.text)\n",
    "#extract title\n",
    "title = soup.find('h3', class_='tit_view')\n",
    "title.get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all('span', class_='txt_info')[0] #call reporter value\n",
    "soup.find_all('span', class_='txt_info')[1] #call date value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract reporter value2\n",
    "info = soup.find('span', class_='info_view')\n",
    "info.find('span', class_='txt_info')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract contents\n",
    "container = soup.find('div', id='harmonyContainer')\n",
    "contents = ''\n",
    "for p in container.find_all('p'):\n",
    "    contents += p.get_text()\n",
    "contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extract tag with CSS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#usign select function\n",
    "soup.select('h3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#contents in specific id\n",
    "# soup.select('#harmonyContainer p') #extract every p in specific id\n",
    "soup.select('#harmonyContainer > p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract value in specific class\n",
    "# soup.select('h3.tit_view')\n",
    "soup.select('.tit_view') #can use this bc there is one class which class is tit_view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract attributes\n",
    "soup.select('h3[class=\"tit_view\"]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# soup.select('h3[class^=\"t\"]') #call every value which starts with 't'\n",
    "# soup.select('h3[class^=\"tx\"]')\n",
    "soup.select('h3[class$=\"view\"]') #call every value which ends with 't'\n",
    "# soup.select('h3[class$=\"_view\"]')\n",
    "soup.select('h3[class*=\"view\"]') #call every value with view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract reporter and date\n",
    "# soup.select('span.txt_info:nth-child(1)') #reporter\n",
    "soup.select('span.txt_info:nth-child(2)') #date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**search tag with regular expressions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract value which starts with h and one number (\\d)\n",
    "soup.find_all(re.compile('h\\d'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract value whose file extension is jpg\n",
    "soup.find_all('img', attrs={'src': re.compile('.+\\.jpg')})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract tag which ends with newsview\n",
    "soup.find_all('h3', class_=re.compile('.+newsview$'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**extract specific value (e.g. number of comments)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* HTTP status code\n",
    " - 1xx(information)\n",
    " - 2xx(success)\n",
    " - 3xx (redirection)\n",
    " - 4xx (client error)\n",
    " - 5xx (server error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://comment.daum.net/apis/v1/ui/single/main/@20190728165812603'\n",
    "resp = requests.get(url)\n",
    "print(resp) #response[401] 4xx means client error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract number of comment count\n",
    "#using developer inspector\n",
    "headers = {\n",
    "    'Authorization': 'Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJmb3J1bV9rZXkiOiJuZXdzIiwiZ3JhbnRfdHlwZSI6ImFsZXhfY3JlZGVudGlhbHMiLCJzY29wZSI6W10sImV4cCI6MTYyMDc1MzgxMywiYXV0aG9yaXRpZXMiOlsiUk9MRV9DTElFTlQiXSwianRpIjoiNjQ4NjczZDktYmIxMy00MWY4LWE3OWEtMWM3ZjAwZTNhMTAxIiwiZm9ydW1faWQiOi05OSwiY2xpZW50X2lkIjoiMjZCWEF2S255NVdGNVowOWxyNWs3N1k4In0.VOn2SV5Odpna5DUEvaz6GO0SdgxkonUHvDdJEM56eRs',\n",
    "    'Origin': 'https://news.v.daum.net',\n",
    "    'Referer': 'https://news.v.daum.net/v/20190728165812603',\n",
    "    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.114 Safari/537.36'\n",
    "}\n",
    "resp = requests.get(url, headers=headers)\n",
    "# print(resp.text)\n",
    "data = resp.json()\n",
    "resp.json()['post']['commentCount']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**crawling data in sites with Login**\n",
    "1. search endpoint (network in developer inspector)\n",
    "2. search form data which transfer id and password\n",
    "3. login generating session object \n",
    "4. crawling with session object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#search login endpoint\n",
    "url = 'https://www.kangcom.com/member/member_check.asp'\n",
    "data = {\n",
    "    'id': 'macmath22',\n",
    "    'pwd': 'Test1357!'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate session \n",
    "s = requests.Session()\n",
    "resp = s.post(url, data = data)\n",
    "print(resp.status_code) #2xx means success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract specific value whose access specific user\n",
    "mypage = 'https://www.kangcom.com/mypage/'\n",
    "resp = s.get(mypage)\n",
    "\n",
    "soup = BeautifulSoup(resp.text)\n",
    "#extract user's miileage\n",
    "# soup.select('td.a_bbslist55')\n",
    "mileage = soup.select_one('td.a_bbslist55:nth-child(3)')\n",
    "print(mileage.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selenium Module\n",
    "* open-source web-based automation tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example for selenium\n",
    "\n",
    "chrome_driver = '/Users/chy/python programming/chromedriver'\n",
    "driver = webdriver.Chrome(chrome_driver)\n",
    "\n",
    "driver.get('https://www.python.org/')\n",
    "\n",
    "search = driver.find_element_by_id('id-search-field')\n",
    "\n",
    "time.sleep(2)\n",
    "search.clear()\n",
    "\n",
    "time.sleep(2)\n",
    "search.send_keys(\"lambda\")\n",
    "\n",
    "time.sleep(2)\n",
    "search.send_keys(Keys.RETURN)\n",
    "\n",
    "time.sleep(2)\n",
    "\n",
    "\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example with news website\n",
    "driver = webdriver.Chrome(chrome_driver)\n",
    "url = 'https://news.v.daum.net/v/20190728165812603'\n",
    "# open the url with chrome driver\n",
    "driver.get(url)\n",
    "\n",
    "src = driver.page_source\n",
    "\n",
    "soup = BeautifulSoup(src)\n",
    "comment_area = soup.select_one('span.alex-count-area')\n",
    "comment_area.get_text()\n",
    "\n",
    "# close the browser\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(chrome_driver)\n",
    "\n",
    "url = 'https://news.naver.com/main/read.nhn?mode=LSD&mid=shm&sid1=105&oid=081&aid=0003018031'\n",
    "driver.get(url)\n",
    "\n",
    "# wait until loading element (time out: 10sec)\n",
    "myElem = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CSS_SELECTOR, 'span.u_cbox_count')))\n",
    "\n",
    "src = driver.page_source\n",
    "\n",
    "soup = BeautifulSoup(src)\n",
    "comment_area = soup.select_one('span.u_cbox_count')\n",
    "\n",
    "driver.close()\n",
    "\n",
    "print(comment_area)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**example crawling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract a news title with news id\n",
    "import requests\n",
    "\n",
    "def get_daum_news_title(news_id):\n",
    "    url = 'https://news.v.daum.net/v/{}'.format(news_id)\n",
    "    resp = requests.get(url)\n",
    "    soup = BeautifulSoup(resp.text)\n",
    "    title_tag = soup.select_one('h3.tit_view')\n",
    "    if title_tag:\n",
    "        return title_tag.get_text()\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_daum_news_title('20190728165812603')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract content\n",
    "def get_daum_news_content(news_id):\n",
    "    url = 'https://news.v.daum.net/v/{}'.format(news_id)\n",
    "    resp = requests.get(url)\n",
    "    soup = BeautifulSoup(resp.text)\n",
    "    content = ''\n",
    "    for p in soup.select('div#harmonyContainer p'):\n",
    "        content += p.get_text()\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_daum_news_content('20190728165812603')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
